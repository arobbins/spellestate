<?php
use Aws\S3\S3Client; // Alias namespace.
// DO NOT CALL THIS CLASS DIRECTLY. CALL VIA: pb_backupbuddy_destination in bootstrap.php.

class pb_backupbuddy_destination_s32 { // Change class name end to match destination name.
	
	const MINIMUM_CHUNK_SIZE = 5; // Minimum size, in MB to allow chunks to be. Anything less will not be chunked even if requested.
	const BACKUP_FILENAME_PATTERN = '/^backup-.*\.zip/i'; //  Used for matching during backup limits, etc to prevent processing non-BackupBuddy files.
	const MAX_AGE_MULTIPART_UPLOADS = 3888000; // Seconds of max age to allow a stalled multipart upload. (72 hrs).
	
	public static $destination_info = array(
		'name'			=>		'Amazon S3 v2',
		'description'	=>		'Amazon S3 is a well known cloud storage provider. This destination is known to be reliable and works well with BackupBuddy. <a href="http://aws.amazon.com/s3/" target="_blank">Learn more here.</a>',
	);
	private static $_client = '';
	private static $_client_signature = '';
	
	// Default settings. Should be public static for auto-merging.
	public static $default_settings = array(
		'type'						=>		's32',		// MUST MATCH your destination slug. Required destination field.
		'title'						=>		'',			// Required destination field.
		
		'accesskey'					=>		'',			// Amazon access key.
		'secretkey'					=>		'',			// Amazon secret key.
		'bucket'					=>		'',			// Amazon bucket to put into.
		
		'directory'					=>		'',			// Subdirectory to put into in addition to the site url directory.
		'ssl'						=>		'1',		// Whether or not to use SSL encryption for connecting.
		'server_encryption'			=>		'AES256',	// Encryption (if any) to have the destination enact. Empty string for none.
		'max_chunk_size'			=>		'80',		// Maximum chunk size in MB. Anything larger will be chunked up into pieces this size (or less for last piece). This allows larger files to be sent than would otherwise be possible. Minimum of 5mb allowed by S3.
		'max_time'					=>		'',			// Default max time in seconds to allow a send to run for. Set to 0 for no time limit. Aka no chunking.
		'max_burst'					=>		'25',		// Max size in mb of each burst within the same page load.
		'db_archive_limit'			=>		'10',		// Maximum number of db backups for this site in this directory for this account. No limit if zero 0.
		'full_archive_limit' 		=>		'4',		// Maximum number of full backups for this site in this directory for this account. No limit if zero 0.
		'files_archive_limit' 		=>		'4',		// Maximum number of files only backups for this site in this directory for this account. No limit if zero 0.
		'manage_all_files'			=>		'1',		// Allow user to manage all files in S3? If enabled then user can view all files after entering their password. If disabled the link to view all is hidden.
		'region'					=>		'us-east-1',	// Region to create buckets in.
		'storage'					=>		'standard',	// Whether to use standard or reduced redundancy storage. Allowed values: STANDARD, REDUCED_REDUNDANCY
		'use_packaged_cert'			=>		'0',		// When 1, use the packaged cacert.pem file included with the AWS SDK.
		'disable_file_management'	=>		'0',		// When 1, _manage.php will not load which renders remote file management DISABLED.
		
		// Do not store these for destination settings. Only used to pass to functions in this file.
		'_multipart_id'				=>		'',			// Instance var. Internal use only for continuing a chunked upload.
		'_multipart_partnumber'		=>		0,			// Instance var. Part number to upload next.
		'_multipart_file'			=>		'',			// Instance var. Internal use only to store the file that is currently set to be multipart chunked.
		'_multipart_remotefile'		=>		'',			// Instance var. Internal use only to store the remote filepath & file.
		'_multipart_counts'			=>		array(),	// Instance var. Multipart chunks to send. Generated by S3's get_multipart_counts().
		'_multipart_transferspeeds'	=>		array(),	// Instance var.
		'_multipart_backup_type'	=>		'',			// Instance var. Type: full, db, files
		'_multipart_backup_size'	=>		'',			// Instance var. Total file size in bytes.
	);
	
	
	
	/* _init()
	 *
	 * Load SDK, create S3 client, prepare bucket, format $settings & return settings.
	 *
	 * @param
	 * @return	array 	Array of formatted and sanitized settings.
	 */
	private static function _init( $settings ) {
		pb_backupbuddy::status( 'details', 'Loading AWS SDK...' );
		require_once( dirname( dirname( __FILE__ ) ) . '/_s3lib2/aws-autoloader.php' );
		pb_backupbuddy::status( 'details', 'SDK loaded.' );
		
		// Format all settings.
		$settings = self::_formatSettings( $settings );
		
		// If not connected with these exact settings (by comparisong signatue of $settings ) then connect & prepare bucket.
		//if ( ! isset( self::$_client ) ) {
		$newSignature = md5( serialize( $settings ) );
		if ( $newSignature != self::$_client_signature ) {
			self::$_client_signature = md5( serialize( $settings ) );
			
			$s3Config = self::getCredentials( $settings );
			if ( '0' == $settings['ssl'] ) {
				$s3Config['scheme'] = 'http';
				pb_backupbuddy::status( 'details', 'SSL disabled.' );
			}
			self::$_client = S3Client::factory( $s3Config );
			
			// Verify bucket exists; create if not. Also set region to the region bucket exists in.
			if ( false === self::_prepareBucketAndRegion( $settings ) ) {
				return self::_error( 'Error #983483437: Could not prepare bucket `' . $settings['bucket'] . '` in region `' . $settings['regision'] . '`.' );
			}
		}
		
		return $settings; // Formatted & updated settings.
		
	} // End _init().
	
	
	
	/* _formatSettings()
	 *
	 * Called by _init().
	 *
	 */
	private static function _formatSettings( $settings ) {
		
		// Format bucket.
		$settings['bucket'] = strtolower( $settings['bucket'] );
		
		// Format directory.
		$path = trim( $settings['directory'], '/\\' );
		if ( $path != '' ) {
			$path .= '/';
		}
		$path = $settings['directory'];
		
		return $settings;
	} // End _formatSettings().
	
	
	
	/*	send()
	 *	
	 *	Send one or more files.
	 *	
	 *	@param		array			$file			Array of one or more files to send.
	 *	@return		boolean|array					True on success, false on failure, array if a multipart chunked send so there is no status yet.
	 */
	public static function send( $settings = array(), $file, $send_id = '', $delete_after = false ) {
		$settings = self::_init( $settings ); // Handles formatting & sanitizing settings.
		
		// Process multipart transfer that we already initiated in a previous runthrough.
		if ( $settings['_multipart_id'] != '' ) { // Multipart upload initiated and needs parts sent.
			
			$backup_type = str_replace( '/', '', $settings['_multipart_backup_type'] ); // For use later by file limiting.
			$backup_size = $settings['_multipart_backup_size'];
			$this_part_number = $settings['_multipart_partnumber'] + 1;
			
			// Open file for streaming.
			$f = @fopen( $settings['_multipart_file'], 'rb' );
			if ( false === $f ) {
				return self::_error( 'Error #437734. Unable to open file `' . $settings['_multipart_file'] . '` to send. Did it get deleted?' );
			}
			if ( -1 == ( @fseek( $f, (integer) $settings['_multipart_counts'][ $settings['_multipart_partnumber'] ]['seekTo'] ) ) ) {
				return self::_error( 'Error #833838: Unable to fseek file.' );
			}
			
			$sendStart = time( true );
			pb_backupbuddy::status( 'details', 'Beginning upload of part `' . $this_part_number . '` of `' . count( $settings['_multipart_counts'] ) . '` parts of file `' . $settings['_multipart_file'] . '` to remote location `' . $settings['_multipart_remotefile'] . '` with multipart ID `' . $settings['_multipart_id'] . '`.' );
			try {
				$response = self::$_client->uploadPart( array(
					'Bucket' => $settings['bucket'],
					'Key' => $settings['_multipart_remotefile'],
					'UploadId' => $settings['_multipart_id'],
					'PartNumber' => $this_part_number,
					'ContentLength' => (integer) $settings['_multipart_counts'][ $settings['_multipart_partnumber'] ]['length'],
					'Body' => $f,
				) );
			} Catch( Exception $e ) {
				@fclose( $f );
				return self::_error( 'Unable to upload file part for multipart upload `' . $settings['_multipart_id'] . '`. Details: `' . $e->getMessage() . '`.' );
			}
			
			@fclose( $f );
			
			pb_backupbuddy::status( 'details', 'Success sending chunk. Upload details: `' . print_r( $response, true ) . '`.' );
			$uploaded_size = $backup_size;
			$uploaded_speed = ( time(true) - $sendStart );
			pb_backupbuddy::status( 'details', 'Uploaded size: ' .  pb_backupbuddy::$format->file_size( $uploaded_size ) . ', Speed: ' . pb_backupbuddy::$format->file_size( $uploaded_speed ) . '/sec.' );
			
			// Load fileoptions to the send.
			pb_backupbuddy::status( 'details', 'About to load fileoptions data.' );
			require_once( pb_backupbuddy::plugin_path() . '/classes/fileoptions.php' );
			pb_backupbuddy::status( 'details', 'Fileoptions instance #10.' );
			$fileoptions_obj = new pb_backupbuddy_fileoptions( backupbuddy_core::getLogDirectory() . 'fileoptions/send-' . $send_id . '.txt', $read_only = false, $ignore_lock = false, $create_file = false );
			if ( true !== ( $result = $fileoptions_obj->is_ok() ) ) {
				return self::_error( __('Fatal Error #9034.2344848. Unable to access fileoptions data.', 'it-l10n-backupbuddy' ) . ' Error: ' . $result );
			}
			pb_backupbuddy::status( 'details', 'Fileoptions data loaded.' );
			$fileoptions = &$fileoptions_obj->options;
			
			
			$update_status = 'Sent part ' . $this_part_number . ' of ' . count( $settings['_multipart_counts'] ) . '.';
			
			
			// Made it here so success sending part. Increment for next part to send.
			$settings['_multipart_partnumber']++;
			
			if ( !isset( $settings['_multipart_counts'][ $settings['_multipart_partnumber'] ] ) ) { // No more parts exist for this file. Tell S3 the multipart upload is complete and move on.
				pb_backupbuddy::status( 'details', 'S3 getting parts with etags to notify S3 of completed multipart send.' );
				
				try {
					$response = self::$_client->listParts( array(
						'Bucket' => $settings['bucket'],
						'UploadId' => $settings['_multipart_id'],
						'Key' => $settings['_multipart_remotefile']
					) );
					$etag_parts = $etag_parts->Parts;
				} Catch( Exception $e ) {
					return self::_error( 'Error #8332893: Unable to list parts on server. Details: `' . $e->getMessage() . '`.' );
				}
				pb_backupbuddy::status( 'details', 'Got parts list. Details: ' . print_r( $etag_parts, true ) );
				pb_backupbuddy::status( 'details', 'Notifying server of multipart upload completion.' );
				
				try {
					$response = self::$_client->completeMultipartUpload( array(
						'Bucket' => $settings['bucket'],
						'UploadId' => $settings['_multipart_id'],
						'Key' => $settings['_multipart_remotefile'],
						'Parts' => $etag_parts 
					) );
				} Catch( Exception $e ) {
					return self::_error( 'Unable to notify server of completion of all parts for multipart upload `' . $settings['_multipart_id'] . '`. Details: `' . $e->getMessage() . '`.' );
				}
				pb_backupbuddy::status( 'details', 'Server notified of multipart completion.' );
				
				pb_backupbuddy::status( 'details', 'No more parts left for this multipart upload. Clearing multipart instance variables.' );
				$settings['_multipart_partnumber'] = 0;
				$settings['_multipart_id'] = '';
				$settings['_multipart_file'] = '';
				$settings['_multipart_remotefile'] = ''; // Multipart completed so safe to prevent housekeeping of incomplete multipart uploads.
				$settings['_multipart_transferspeeds'][] = $uploaded_speed;
				
				// Overall upload speed average.
				$uploaded_speed = array_sum( $settings['_multipart_transferspeeds'] ) / count( $settings['_multipart_counts'] );
				pb_backupbuddy::status( 'details', 'Upload speed average of all chunks: `' . pb_backupbuddy::$format->file_size( $uploaded_speed ) . '`.' );
				
				$settings['_multipart_counts'] = array();
				
				// Update stats.
				$fileoptions['_multipart_status'] = $update_status;
				$fileoptions['finish_time'] = time();
				$fileoptions['status'] = 'success';
				if ( isset( $uploaded_speed ) ) {
					$fileoptions['write_speed'] = $uploaded_speed;
				}
				$fileoptions_obj->save();
				unset( $fileoptions );
			}
			
			
			
			// Schedule to continue if anything is left to upload for this multipart of any individual files.
			if ( $settings['_multipart_id'] != '' ) {
				pb_backupbuddy::status( 'details', 'S3 multipart upload has more parts left. Scheduling next part send.' );
				
				$cronTime = time();
				$cronArgs = array( $settings, $file, $send_id, $delete_after );
				$cronHashID = md5( $cronTime . serialize( $cronArgs ) );
				$cronArgs[] = $cronHashID;
				
				$schedule_result = backupbuddy_core::schedule_single_event( $cronTime, pb_backupbuddy::cron_tag( 'destination_send' ), $cronArgs );
				if ( true === $schedule_result ) {
					pb_backupbuddy::status( 'details', 'Next S3 chunk step cron event scheduled.' );
				} else {
					pb_backupbuddy::status( 'error', 'Next S3 chunk step cron even FAILED to be scheduled.' );
				}
				spawn_cron( time() + 150 ); // Adds > 60 seconds to get around once per minute cron running limit.
				update_option( '_transient_doing_cron', 0 ); // Prevent cron-blocking for next item.
				
				return array( $settings['_multipart_id'], 'Sent part ' . $this_part_number . ' of ' . count( $settings['_multipart_counts'] ) . ' parts.' );
			}
		} else { // not multipart continuation
			
			// Handle chunking of file into a multipart upload (if applicable).
			$file_size = filesize( $file );
			if ( ( $settings['max_chunk_size'] >= self::MINIMUM_CHUNK_SIZE ) && ( ( $file_size / 1024 / 1024 ) > $settings['max_chunk_size'] ) ) { // minimum chunk size is 5mb. Anything under 5mb we will not chunk.
				
				pb_backupbuddy::status( 'details', 'File size of ' . pb_backupbuddy::$format->file_size( $file_size ) . ' exceeds max chunk size of ' . $settings['max_chunk_size'] . 'MB set in settings for sending file as multipart upload.' );
				
				// About to chunk so cleanup any previous hanging multipart transfers.
				self::multipart_cleanup( $settings, $lessLogs = false );
				
				// Initiate multipart upload with S3.
				pb_backupbuddy::status( 'details', 'Initiating multipart transfer.' );
				try {
					$response = self::$_client->createMultipartUpload( array(
						'Bucket' => $settings['bucket'],
						'Key' => $settings['directory'] . basename( $file ),
						'StorageClass' => $settings['storage'],
						'ServerSideEncryption' => 'AES256',
					 ) );
				} Catch( Exception $e ) {
					return self::_error ( 'Error #389383: Unable to initiate multipart upload. Details: `' . $e->getMessage() . '`.' );
				}
				
				// Made it here so SUCCESS initiating multipart!
				$upload_id = (string) $response['UploadId'];
				pb_backupbuddy::status( 'details', 'Initiated multipart upload with ID `' . $upload_id . '`.' );
				
				$backup_type = backupbuddy_core::getBackupTypeFromFile( $file );
				
				// Calculate multipart settings.
				$multipart_destination_settings = $settings;
				$multipart_destination_settings['_multipart_id'] = $upload_id;
				$multipart_destination_settings['_multipart_partnumber'] = 0;
				$multipart_destination_settings['_multipart_file'] = $file;
				$multipart_destination_settings['_multipart_remotefile'] = $settings['directory'] . basename( $file );
				$multipart_destination_settings['_multipart_counts'] = self::_get_multipart_counts( $file_size, $settings['max_chunk_size'] * 1024 * 1024 ); // Size of chunks expected to be in bytes.
				$multipart_destination_settings['_multipart_backup_type'] = $backup_type;
				$multipart_destination_settings['_multipart_backup_size'] = $file_size;
				
				pb_backupbuddy::status( 'details', 'Multipart settings to pass:' . print_r( $multipart_destination_settings, true ) );
				
				// Schedule to process the parts.
				pb_backupbuddy::status( 'details', 'Scheduling send of next part.' );
				
				$cronTime = time();
				$cronArgs = array( $multipart_destination_settings, $file, $send_id, $delete_after );
				$cronHashID = md5( $cronTime . serialize( $cronArgs ) );
				$cronArgs[] = $cronHashID;
				
				backupbuddy_core::schedule_single_event( $cronTime, pb_backupbuddy::cron_tag( 'destination_send' ), $cronArgs );
				spawn_cron( time() + 150 ); // Adds > 60 seconds to get around once per minute cron running limit.
				update_option( '_transient_doing_cron', 0 ); // Prevent cron-blocking for next item.
				pb_backupbuddy::status( 'details', 'Scheduled send of next part(s). Done for this cycle.' );
				
				return array( $upload_id, 'Starting send of ' . count( $multipart_destination_settings['_multipart_counts'] ) . ' parts.' );
			} else { // did not meet chunking criteria.
				if ( $settings['max_chunk_size'] != '0' ) {
					if ( ( $file_size / 1024 / 1024 ) > self::MINIMUM_CHUNK_SIZE ) {
						pb_backupbuddy::status( 'details', 'File size of ' . pb_backupbuddy::$format->file_size( $file_size ) . ' is less than the max chunk size of ' . $settings['max_chunk_size'] . 'MB; not chunking into multipart upload.' );
					} else {
						pb_backupbuddy::status( 'details', 'File size of ' . pb_backupbuddy::$format->file_size( $file_size ) . ' is less than the minimum allowed chunk size of ' . self::MINIMUM_CHUNK_SIZE . 'MB; not chunking into multipart upload.' );
					}
				} else {
					pb_backupbuddy::status( 'details', 'Max chunk size set to zero so not chunking into multipart upload.' );
				}
				
				// Open file for streaming.
				$f = @fopen( $file, 'rb' );
				if ( false === $f ) {
					return self::_error( 'Error #2379327. Unable to open file `' . $file . '` to send. Did it get deleted?' );
				}
				
				// Initiate SINGLE PART upload.
				$startSend = time( true );
				pb_backupbuddy::status( 'details', 'Initiating non-chunked transfer.' );
				try {
					$response = self::$_client->upload(
						$settings['bucket'],
						$settings['directory'] . basename( $file ),
						$f,
						'private',
						array(
							'StorageClass' => $settings['storage'],
							'ServerSideEncryption' => 'AES256',
						)
					);
				} Catch( Exception $e ) {
					return self::_error ( 'Error #389383: Unable to initiate non-chunked upload. Details: `' . $e->getMessage() . '`.' );
				}
				
				@fclose( $f );
				
				// Made it here so SUCCESS sending.
				$uploaded_size = $file_size;
				$uploaded_speed = $file_size / ( time(true) - $startSend );
				pb_backupbuddy::status( 'details', 'Success uploading file `' . basename( $file ) . '`. Upload details: `' . print_r( $response, true ) . '`. Uploaded size: ' .  pb_backupbuddy::$format->file_size( $uploaded_size ) . ', Speed: ' . pb_backupbuddy::$format->file_size( $uploaded_speed ) . '/sec.' );
				
				// Load destination fileoptions.
				pb_backupbuddy::status( 'details', 'About to load fileoptions data.' );
				require_once( pb_backupbuddy::plugin_path() . '/classes/fileoptions.php' );
				pb_backupbuddy::status( 'details', 'Fileoptions instance #882.' );
				$fileoptions_obj = new pb_backupbuddy_fileoptions( backupbuddy_core::getLogDirectory() . 'fileoptions/send-' . $send_id . '.txt', $read_only = false, $ignore_lock = false, $create_file = false );
				if ( true !== ( $result = $fileoptions_obj->is_ok() ) ) {
					return self::_error( __('Fatal Error #9034.23737. Unable to access fileoptions data.', 'it-l10n-backupbuddy' ) . ' Error: ' . $result );
				}
				pb_backupbuddy::status( 'details', 'Fileoptions data loaded.' );
				$fileoptions = &$fileoptions_obj->options;
				
				// Save stats.
				if ( isset( $uploaded_speed ) ) {
					$fileoptions['write_speed'] = $uploaded_speed;
					$fileoptions_obj->save();
				}
				unset( $fileoptions_obj );
			} // End non-chunked upload.
			
		} // end not multipart continuation.
		
		
		// BEGIN FILE LIMIT PROCESSING. Enforce archive limits if applicable.
		if ( $backup_type == 'full' ) {
			$limit = $full_archive_limit;
			pb_backupbuddy::status( 'details', 'Full backup archive limit of `' . $limit . '` of type `full` based on destination settings.' );
		} elseif ( $backup_type == 'db' ) {
			$limit = $db_archive_limit;
			pb_backupbuddy::status( 'details', 'Database backup archive limit of `' . $limit . '` of type `db` based on destination settings.' );
		} elseif ( $backup_type == 'files' ) {
			$limit = $db_archive_limit;
			pb_backupbuddy::status( 'details', 'Database backup archive limit of `' . $limit . '` of type `files` based on destination settings.' );
		} else {
			$limit = 0;
			pb_backupbuddy::status( 'warning', 'Warning #237332. Unable to determine backup type (reported: `' . $backup_type . '`) so archive limits NOT enforced for this backup.' );
		}
		if ( $limit > 0 ) {
			
			pb_backupbuddy::status( 'details', 'Archive limit enforcement beginning.' );
			
			// Get file listing.
			try {
				$response_manage = self::$_client->listObjects( array(
					'Bucket' => $settings['bucket'],
					'Prefix' => $settings['directory'] . 'backup-' . backup_prefix()
				) ); // List all users files in this directory that are a backup for this site (limited by prefix).
			} Catch( Exception $e ) {
				self::_error( 'Error #9338292: Unable to list files for archive limiting. Details: `' . $e->getMessage() . '`.' );
			}
			
			// List backups associated with this site by date.
			$backups = array();
			foreach( $response_manage->Contents as $object ) {
				$file = str_replace( $settings['directory'], '', $object['Key'] );
				$backups[$file] = strtotime( $object['LastModified'] );
			}
			arsort( $backups );
			
			
			pb_backupbuddy::status( 'details', 'Stash found `' . count( $backups ) . '` backups of this type when checking archive limits.' );
			if ( ( count( $backups ) ) > $limit ) {
				pb_backupbuddy::status( 'details', 'More archives (' . count( $backups ) . ') than limit (' . $limit . ') allows. Trimming...' );
				$i = 0;
				$delete_fail_count = 0;
				foreach( $backups as $buname => $butime ) {
					$i++;
					if ( $i > $limit ) {
						pb_backupbuddy::status( 'details', 'Trimming excess file `' . $buname . '`...' );
						try {
							$response = self::$_client->deleteObject( array(
								'Bucket' => $settings['bucket'],
								'Key' => $settings['directory'] . $buname,
							) );
						} Catch( Exception $e ) {
							self::_error( 'Unable to delete excess Stash file `' . $buname . '`. Details: `' . $e->getMessage() . '`.' );
							$delete_fail_count++;
						}
					}
				}
				pb_backupbuddy::status( 'details', 'Finished trimming excess backups.' );
				if ( $delete_fail_count !== 0 ) {
					$error_message = 'Stash remote limit could not delete ' . $delete_fail_count . ' backups.';
					pb_backupbuddy::status( 'error', $error_message );
					backupbuddy_core::mail_error( $error_message );
				}
			}
			
			pb_backupbuddy::status( 'details', 'Stash completed archive limiting.' );
			
		} else {
			pb_backupbuddy::status( 'details',  'No Stash archive file limit to enforce.' );
		} // End remote backup limit
		
		
		if ( isset( $fileoptions_obj ) ) {
			unset( $fileoptions_obj );
		}
		
		// Success if we made it this far.
		return true;
		
	} // End send().
	
	
	
	public static function listFiles( $settings, $prefix ) {
		$settings = self::_init( $settings );
		
		try {
			$response = self::$_client->listObjects( array(
				'Bucket' => $settings['bucket'],
				'Prefix' => $prefix
			) ); // list all the files in the subscriber account
		} Catch( Exception $e ) {
			return self::_error( 'Error #838393: Unable to list files. Details: `' . $e->getMessage() . '`.' );
		}
		
		return $response['Contents'];
		
	} // End listFiles().
	
	
	public static function deleteFile( $settings, $file ) {
		return self::deleteFiles( $settings, array( $file ) );
	}
	
	
	public static function deleteFiles( $settings, $files ) {
		$settings = self::_init( $settings );
		
		$fileKeys = array();
		foreach( $files as $file ) {
			$fileKeys[] = array(
				'Key' => $settings['directory'] . $file
			);
		}
		
		try {
			$response = self::$_client->deleteObjects( array(
				'Bucket' => $settings['bucket'],
				'Objects' => $fileKeys
			) ); // list all the files in the subscriber account
		} Catch( Exception $e ) {
			return self::_error( 'Error #838393: Unable to delete one or more files. Details: `' . $e->getMessage() . '`.' );
		}
		
		return true;
		
	}
	
	/*	test()
	 *	
	 *	Tests ability to write to this remote destination.
	 *	
	 *	@param		array			$settings	Destination settings.
	 *	@return		bool|string					True on success, string error message on failure.
	 */
	public static function test( $settings ) {
		$settings = self::_init( $settings );
		
		// Try sending a file.
		$send_response = pb_backupbuddy_destinations::send( $settings, dirname( dirname( __FILE__ ) ) . '/remote-send-test.php', $send_id = 'TEST-' . pb_backupbuddy::random_string( 12 ) ); // 3rd param true forces clearing of any current uploads.
		if ( false === $send_response ) {
			$send_response = 'Error sending test file to S3.';
		} else {
			$send_response = 'Success.';
		}
			
		// Delete sent file.
		$delete_response = 'Success.';
		try {
			$delete_response = self::$_client->delete_object( array(
				'Bucket' => $credentials['bucket'],
				'Key' => $settings['directory'] . 'remote-send-test.php'
			) );
			$delete_response = 'Success.';
		} Catch( Exception $e ) {
			pb_backupbuddy::status( 'details', 'Unable to delete test S3 file `remote-send-test.php`. Details: `' . $e->getMessage() . '`.' );
		}
		
		// Load destination fileoptions.
		pb_backupbuddy::status( 'details', 'About to load fileoptions data.' );
		require_once( pb_backupbuddy::plugin_path() . '/classes/fileoptions.php' );
		pb_backupbuddy::status( 'details', 'Fileoptions instance #7.' );
		$fileoptions_obj = new pb_backupbuddy_fileoptions( backupbuddy_core::getLogDirectory() . 'fileoptions/send-' . $send_id . '.txt', $read_only = false, $ignore_lock = false, $create_file = false );
		if ( true !== ( $result = $fileoptions_obj->is_ok() ) ) {
			return self::_error( __('Fatal Error #9034.84838. Unable to access fileoptions data.', 'it-l10n-backupbuddy' ) . ' Error: ' . $result );
		}
		pb_backupbuddy::status( 'details', 'Fileoptions data loaded.' );
		$fileoptions = &$fileoptions_obj->options;
		
		if ( ( 'Success.' != $send_response ) || ( 'Success.' != $delete_response ) ) {
			$fileoptions['status'] = 'failure';
			
			$fileoptions_obj->save();
			unset( $fileoptions_obj );
			
			return 'Send details: `' . $send_response . '`. Delete details: `' . $delete_response . '`.';
		} else {
			$fileoptions['status'] = 'success';
			$fileoptions['finish_time'] = time();
		}
		
		$fileoptions_obj->save();
		unset( $fileoptions_obj );
		
		return true;
		
	} // End test().
	
	
	
	/* download_file()
	 *
	 * Download remote file to local system.
	 *
	 * @param	array 		$settings				Destination settings.
	 * @param	string		$remoteFile				Remote filename.
	 * @param	string		$localDestinationFile	Full path & filename of destination file.
	 *
	 */
	public static function download_file( $settings, $remoteFile, $localDestinationFile ) {
		$settings = self::_init( $settings );
		
		pb_backupbuddy::status( 'details', 'Downloading remote file `' . $remoteFile . '` from S3 to local file `' . $localDestinationFile . '`.' );
		
		try {
			$response = self::$_client->get_object( array(
				'Bucket' => $settings['bucket'],
				'Key' => $remotePath . $remoteFile,
				'SaveAs' => $localDestinationFile
			) );
		} Catch( Exception $e ) {
			return self::_error( 'Error #382938: Unable to retrieve file. Details: `' . $e->getMessage() . '`.' );
		}
		
		return true;
		
	} // end download_file().
	
	
	
	/* getFileURL()
	 *
	 * Get download URL for a file.
	 *
	 * @param	array 	$settings		Destination settings.
	 * @param	string	$remoteFile		Filename of remote file. Does NOT contain path as it is calculated from $settings.
	 * @param	int		$expires		Timestamp to expire. Defaults to 1 hour.
	 * @return	
	 *
	 */
	public static function getFileURL( $settings, $remoteFile, $expires = 0 ) {
		$settings = self::_init( $settings );
		
		pb_backupbuddy::status( 'details', 'Getting download URL.' );
		if ( ( 0 == $expires ) || ( ! is_numeric( $expires ) ) ) {
			$expires = time() + 3600; // 1 hour default.
		}
		
		try {
			$response = self::$_client->getObjectUrl( $settings['bucket'], $settings['directory'] . $remoteFile, $expires );
		} Catch( Exception $e ) {
			return self::_error( 'Error #349734634: Unable to retrieve file URL. Details: `' . $e->getMessage() . '`.' );
		}
		
		return $response;
	} // End getFileURL().
	
	
	
	/*	getCredentials()
	 *	
	 *	Get the required credentials and management data for managing user files.
	 *	
	 *	@return		false|array			Boolean false on failure. Array of data on success.
	 */
	public static function getCredentials( $settings ) {
		$settings['bucket'] = strtolower( $settings['bucket'] ); // Buckets must be lowercase.
		
		$credentials = array(
			'bucket'	=> $settings['bucket'],
			'key' 		=> $settings['accesskey'],
			'secret'	=> $settings['secretkey'],
		);

		if ( '1' == $settings['use_packaged_cert'] ) {
			pb_backupbuddy::status( 'details', 'Using packaged cacert.pem file based on destination settings.' );
			$credentials['ssl.certificate_authority'] = pb_backupbuddy::plugin_path() . '/destinations/_s3lib/aws-sdk/lib/requestcore/cacert.pem';
		}
		
		return $credentials;
		
	} // End getCredentials().
	
	
	
	public static function get_bucket_region( $bucket ) {
		try {
			$result = self::$_client->getBucketLocation(array(
				'Bucket' => $bucket,
			));
		} catch (Exception $e) {
			throw new Exception( $e->getMessage() );
		}
		
		if ( ! isset( $result->Location ) ) { // Blank means default standard location.
			return 'us-east-1';
		}
		
		$location = $result->Location;
		if ( 'EU' == $location ) {
			$location = 'eu-west-1';
		}
		
		return $location;
	}
	
	
	
	/* multipart_cleanup()
	 *
	 * S3 does NOT automatically clean up failred or expired multipart chunk files so clean up for them.
	 *
	 */
	public static function multipart_cleanup( $settings, $lessLogs = true ) {
		$settings = self::_init( $settings ); // Handles formatting & sanitizing settings.
		
		pb_backupbuddy::status( 'details', 'AWS Multipart cleanup beginning.' );
		
		try {
			$response = self::$_client->listMultipartUploads( array(
				'Bucket' => $settings['bucket'],
				'prefix' => 'backup',
			) );
		} Catch( Exception $e ) {
			return self::_error( 'Error #84397849347: Unable to list existing multipart uploads. Details: `' . $e->getMessage() . '`' );
		}
		
		if ( true !== $lessLogs ) {
			pb_backupbuddy::status( 'details', 'Multipart upload check retrieved. Found `' . count( $response['Uploads'] ) . '` multipart uploads in progress / stalled. Details: `' . print_r( $response, true ) . '`' );
		} else {
			pb_backupbuddy::status( 'details', 'Multipart upload check retrieved. Found `' . count( $response['>Uploads'] ) . '` multipart uploads in progress / stalled. Old BackupBuddy parts will be cleaned up (if any found) ...' );
		}
		
		// Loop through each incomplete multipart upload.
		foreach( $response['Uploads'] as $upload ) {
			if ( true !== $lessLogs ) {
				pb_backupbuddy::status( 'details', 'Checking upload: ' . print_r( $upload, true ) );
			}
			if ( FALSE !== stristr( $upload['Key'], 'backup-' ) ) { // BackupBuddy backup file.
				$initiated = strtotime( $upload['Initiated'] );
				if ( true !== $lessLogs ) {
					pb_backupbuddy::status( 'details', 'Multipart Chunked Upload(s) detected in progress. Age: `' . pb_backupbuddy::$format->time_ago( $initiated ) . '`.' );
				}
				
				// If too old then cancel it.
				if ( ( $initiated + self::MAX_AGE_MULTIPART_UPLOADS ) < time() ) {
					
					try {
						self::$_client->abortMultipartUpload( $settings['bucket'], $upload['Key'], $upload['UploadId'] );
						pb_backupbuddy::status( 'details', 'Stalled Multipart Chunked Uploads ABORTED ID `' . $upload->UploadId . '` of age `' . pb_backupbuddy::$format->time_ago( $initiated ) . '`.' );
					} Catch( Exception $e ) {
						pb_backupbuddy::status( 'error', 'Stalled Multipart Chunked abort of file `' . $upload['Key'] . '` with ID `' . $upload['UploadId'] . '` FAILED. Manually abort it. Details: `' . $e->getMessage() . '`.' );
					}
					
				} else {
					if ( true !== $lessLogs ) {
						pb_backupbuddy::status( 'details', 'Multipart Chunked Uploads not aborted as not too old.' );
					}
				}
			}
		}
		
		pb_backupbuddy::status( 'details', 'AWS Multipart cleanup finished.' );
		return true;
		
	} // end multipart_cleanup().
	
	
	
	/* _prepareBucketAndRegion()
	 *
	 * Validates bucket existance, creating if needed.  Sets region for non-US usage.
	 *
	 * @param	array 		$settings		Destination settings array.
	 * @param	bool		$createBucket	Whether or not to create bucket if it does not currently exist.
	 * @return	bool						true on all okay, false otherwise.
	 *
	 */
	private static function _prepareBucketAndRegion( $settings, $createBucket = true ) {
		$settings = self::_init( $settings ); // Handles formatting & sanitizing settings.
		
		// Get bucket region to determine if a bucket already exists.
		// Assume we will not have to try and create a bucket
		$maybe_create_bucket = false;
		pb_backupbuddy::status( 'details', 'Getting region for bucket: `' . $settings['bucket'] . "`." );
		
		try {
			$detectedRegion = self::get_bucket_region( $settings['bucket'] );
			$result = self::$_client->getBucketLocation(array(
				'Bucket' => $settings['bucket'],
			));
			pb_backupbuddy::status( 'details', 'Server indicates region: ' . $detectedRegion );
			$settings['region'] = $detectedRegion; // Override passed region.
		} catch( Exception $e ) {
			$detectedRegion = '';
			$maybe_create_bucket = true;
			$message = 'Exception retrieving information for bucket `' . $settings['bucket'] . '`. Assuming region in $settings correct. If using IAM security, verify this resource ALLOWs the action "s3:GetBucketLocation". Details: `' . $e->getMessage() . '`.';
			echo $message;
			pb_backupbuddy::status( 'warning', $message );
			//self::_error ( $message );
		}
		
		try {
			self::$_client->setRegion( $settings['region'] );
		} catch( Exception $e ) {
			throw new Exception( 'Unable to set region. Details: `' . $e->getMessage() . '`.' );
		}
		
		// In bucket creation mode AND bucket did not already exist.
		if ( ( true === $createBucket ) && ( true === $maybe_create_bucket ) ) {
			
			pb_backupbuddy::status( 'details', 'Attempting to create bucket `' . $settings['bucket'] . '` at region endpoint `' . $settings['region'] . '` (detected region: `' . $detectedRegion . '`).' );
			try {
				$response = self::$_client->createBucket( array(
					'ACL' => 'private',
					'Bucket' => $settings['bucket'],
					'LocationConstraint' => $settings['region']
				) );
			} catch( Exception $e ) {
				return self::_error( 'Error #3892833: Unable to create bucket. Details: `' . $e->getMessage . '`.' );
			}
			
		} // end if create bucket.
		
		return true;
		
	} // end _prepareBucketAndRegion().
	
	
	// Taken from v1 s3 SDK.
	private static function _get_multipart_counts($filesize, $part_size)
	{
		$i = 0;
		$sizecount = $filesize;
		$values = array();

		while ($sizecount > 0)
		{
			$sizecount -= $part_size;
			$values[] = array(
				'seekTo' => ($part_size * $i),
				'length' => (($sizecount > 0) ? $part_size : ($sizecount + $part_size)),
			);
			$i++;
		}

		return $values;
	}
	
	private static function _error( $message ) {
		global $pb_backupbuddy_destination_errors;
		$pb_backupbuddy_destination_errors[] = $message;
		pb_backupbuddy::status( 'error', $message );
		return false;
	}
	
} // End class.